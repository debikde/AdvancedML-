{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:50:07.358762Z",
     "start_time": "2025-04-16T15:49:59.123844Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "\n",
    "import tokenizer\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from datasets import load_dataset\n",
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ],
   "id": "ca0c7fb637ae592f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T10:31:13.092011Z",
     "start_time": "2025-04-15T10:31:13.084393Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class InputEmbeddingNew(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n"
   ],
   "id": "6903a2d692c7fe40",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Создаём эмбеддинг-слой, который превращает номер слова в вектор фиксированной размерности `d_model`\n",
    "Это улучшает обучение модели (так в статье)\n",
    "\n"
   ],
   "id": "6b051b41399699ef"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:50.913214Z",
     "start_time": "2025-04-10T12:05:50.900572Z"
    }
   },
   "source": [
    "class PositionalEncodingNew(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model) #Таблица [макс дина x размерность], записываем информацию о позициях.\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)].detach()\n",
    "        return self.dropout(x)"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Добавляет информацию о позиции токена во входной последовательности с помощью синусоид.\n",
    "\n",
    "## Формулы:\n",
    "PE(pos, 2i) = sin(pos / 10000^(2i/d_model)) \n",
    "\n",
    "PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
    "\n",
    "Где:\n",
    "- `pos` — позиция токена.\n",
    "- `i` — индекс размерности.\n",
    "\n",
    "Позволяет модели использовать положение слов."
   ],
   "id": "c957884a89b19f30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:50.945438Z",
     "start_time": "2025-04-10T12:05:50.942313Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "170326089c87fe53",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:50.950501Z",
     "start_time": "2025-04-10T12:05:50.945438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "ffcefc23cdcfb98f",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "Обычная нейросеть: два линейных слоя, с ReLU посередине.\n",
    " \n",
    "## Формула:\n",
    "\n",
    "FFN(x) = max(0, xW₁ + b₁)W₂ + b₂\n",
    "\n",
    "- Первая линейная трансформация увеличивает размерность до `d_ff`, затем применяется ReLU, и возвращается к `d_model`."
   ],
   "id": "f0124191e42ed1b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:50.961360Z",
     "start_time": "2025-04-10T12:05:50.950501Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % heads == 0\n",
    "\n",
    "        self.d_k = d_model // heads\n",
    "        self.heads = heads\n",
    "\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        B, L, _ = q.shape\n",
    "\n",
    "        def transform(x, layer):\n",
    "            x = layer(x)\n",
    "            x = x.view(B, -1, self.heads, self.d_k).transpose(1, 2)\n",
    "            return x\n",
    "\n",
    "        q = transform(q, self.w_q)\n",
    "        k = transform(k, self.w_k)\n",
    "        v = transform(v, self.w_v)\n",
    "\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k) #Считаем, насколько слово q связано со словом k. Делим на корень, чтобы числа не разлетались\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "#Превращаем оценки в вероятности \n",
    "        context = torch.matmul(attn, v) \n",
    "        context = context.transpose(1, 2).contiguous().view(B, -1, self.heads * self.d_k)\n",
    "        return self.w_o(context)"
   ],
   "id": "ef2008dadee62642",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Позволяет модели \"смотреть\" на разные части входа одновременно с разных \"углов зрения\".\n",
    "\n",
    "## Формулы:\n",
    "1. Входные преобразования:\n",
    "Q = XW^Q, K = XW^K, V = XW^V\n",
    "\n",
    "\n",
    "2. Считаем \"внимание\":\n",
    "Attention(Q, K, V) = softmax(QKᵀ / sqrt(d_k)) V\n",
    "\n",
    "\n",
    "3. Разделение на головы (h голов):\n",
    "MultiHead(Q, K, V) = Concat(head₁, ..., head_h) W^O\n",
    "\n",
    "где:\n",
    "\n",
    "head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\n",
    "- Каждая голова работает с проекциями меньшей размерности (d_k = d_model / heads).\n",
    "- Маска используется для запрета определенных позиций"
   ],
   "id": "e4bf99fcdf9aefb1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:50.967372Z",
     "start_time": "2025-04-10T12:05:50.963364Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "f77e38edaba88fca",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:50.978987Z",
     "start_time": "2025-04-10T12:05:50.967372Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout(self.self_attn(x2, x2, x2, mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout(self.ff(x2))\n",
    "        return x\n"
   ],
   "id": "af6d893886dc8686",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Один слой энкодера = self-attention + feed-forward.\n",
    "\n",
    "- Нормализация перед каждым блоком (pre-norm).\n",
    "- Остаточные связи стабилизируют обучение."
   ],
   "id": "316a303e5193d8e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:50.989046Z",
     "start_time": "2025-04-10T12:05:50.978987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N, d_model):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)"
   ],
   "id": "56dc84854146429d",
   "outputs": [],
   "execution_count": 56
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "Последовательное применение `N` энкодер-блоков.\n",
    "\n",
    "Каждый блок применяет self-attention и feed-forward над входом, и результат нормализуется в конце.\n"
   ],
   "id": "88a9a1bd3dbb4726"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:50.998118Z",
     "start_time": "2025-04-10T12:05:50.989046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, heads, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, heads, dropout)\n",
    "        self.ff = FeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        x2 = self.norm1(x)\n",
    "        x = x + self.dropout(self.self_attn(x2, x2, x2, tgt_mask))\n",
    "        x2 = self.norm2(x)\n",
    "        x = x + self.dropout(self.cross_attn(x2, memory, memory, src_mask))\n",
    "        x2 = self.norm3(x)\n",
    "        x = x + self.dropout(self.ff(x2))\n",
    "        return x\n",
    " "
   ],
   "id": "f23290cc21b96263",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Один слой декодера. Включает:\n",
    "1. Masked self-attention\n",
    "2. Cross-attention (с encoder output)\n",
    "3. Feed-forward\n",
    "\n",
    "Self-attention (только на предыдущие слова)\n",
    "\n",
    "Cross-attention (на выход энкодера)\n",
    "\n",
    "x = x + Dropout(SelfAttention(LayerNorm(x))) x = x + Dropout(CrossAttention(LayerNorm(x), memory)) x = x + Dropout(FeedForward(LayerNorm(x)))\n",
    "\n",
    "- `masked self-attention`: маскирует будущие токены.\n",
    "- `cross-attention`: позволяет декодеру использовать информацию из энкодера."
   ],
   "id": "b160ef25c1fd1700"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:51.027073Z",
     "start_time": "2025-04-10T12:05:51.020206Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N, d_model):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([layer for _ in range(N)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ],
   "id": "60847afc51c398dd",
   "outputs": [],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Применяет `N` блоков декодера\n",
    "\n",
    ".\n",
    "На каждом шаге декодирования используется как сам ввод, так и закодированное представление из энкодера."
   ],
   "id": "c2d6aaebeccdba2b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:51.043203Z",
     "start_time": "2025-04-10T12:05:51.027073Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim=-1)"
   ],
   "id": "2e436056142db8ee",
   "outputs": [],
   "execution_count": 59
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Проецирует выход декодера на размерность словаря и применяет логарифм от softmax.\n",
    "\n",
    "## Формула:\n",
    "P(y_t | ...) = log_softmax(Linear(output), dim=-1)\n",
    "\n",
    "Где:\n",
    "- `Linear(output)` — логиты по словарю.\n",
    "- `log_softmax` — логарифм вероятностей для обучения с использованием `NLLLoss`.\n",
    "\n",
    "На выходе — логарифм вероятностей каждого слова в словаре."
   ],
   "id": "3f64918043dc28ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-10T12:05:51.058155Z",
     "start_time": "2025-04-10T12:05:51.043203Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, N=6, heads=8, d_ff=2048, dropout=0.1, max_len=5000, share_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_embed = nn.Sequential(\n",
    "            InputEmbedding(d_model, src_vocab_size),\n",
    "            PositionalEncoding(d_model, max_len, dropout)\n",
    "        )\n",
    "\n",
    "        self.tgt_embed = nn.Sequential(\n",
    "            InputEmbedding(d_model, tgt_vocab_size),\n",
    "            PositionalEncoding(d_model, max_len, dropout)\n",
    "        )\n",
    "\n",
    "        encoder_block = EncoderBlock(d_model, heads, d_ff, dropout)\n",
    "        decoder_block = DecoderBlock(d_model, heads, d_ff, dropout)\n",
    "\n",
    "        self.encoder = Encoder(encoder_block, N, d_model)\n",
    "        self.decoder = Decoder(decoder_block, N, d_model)\n",
    "\n",
    "        self.projection = ProjectionLayer(d_model, tgt_vocab_size)\n",
    "\n",
    "        #(если размеры совпадают)\n",
    "        if share_weights and src_vocab_size == tgt_vocab_size:\n",
    "            self.tgt_embed[0].embedding.weight = self.projection.proj.weight\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def encode(self, src, src_mask):\n",
    "        x = self.src_embed(src)\n",
    "        return self.encoder(x, src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, src_mask, tgt_mask):\n",
    "        x = self.tgt_embed(tgt)\n",
    "        return self.decoder(x, memory, src_mask, tgt_mask)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        memory = self.encode(src, src_mask)\n",
    "        out = self.decode(tgt, memory, src_mask, tgt_mask)\n",
    "        return self.projection(out)\n",
    "        "
   ],
   "id": "2b9244072423ef8a",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Компоненты:\n",
    "1. Эмбеддинг + позиционная энкодировка (src & tgt).\n",
    "2. Энкодер: `N` EncoderBlock.\n",
    "3. Декодер: `N` DecoderBlock.\n",
    "4. Линейная проекция в словарь.\n",
    "\n",
    "\n"
   ],
   "id": "973d482fcae5b391"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:50:24.601769Z",
     "start_time": "2025-04-16T15:50:24.581446Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1).float()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=512, nhead=8, num_encoder_layers=6,\n",
    "                 num_decoder_layers=6, dim_feedforward=1024, dropout=0.1, max_len=5000, share_weights=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.src_embed = nn.Sequential(\n",
    "            InputEmbedding(d_model, src_vocab_size),\n",
    "            PositionalEncoding(d_model, max_len, dropout)\n",
    "        )\n",
    "\n",
    "        self.tgt_embed = nn.Sequential(\n",
    "            InputEmbedding(d_model, tgt_vocab_size),\n",
    "            PositionalEncoding(d_model, max_len, dropout)\n",
    "        )\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=num_encoder_layers,\n",
    "            num_decoder_layers=num_decoder_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True \n",
    "        )\n",
    "\n",
    "        self.projection = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        if share_weights and src_vocab_size == tgt_vocab_size:\n",
    "            self.tgt_embed[0].embedding.weight = self.projection.weight\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for p in self.parameters():\n",
    "            if p.dim() > 1:\n",
    "                nn.init.xavier_uniform_(p)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None,\n",
    "                src_key_padding_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        src_emb = self.src_embed(src)\n",
    "        tgt_emb = self.tgt_embed(tgt)\n",
    "\n",
    "        output = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            src_mask=src_mask,\n",
    "            tgt_mask=tgt_mask,\n",
    "            memory_mask=memory_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "            tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "            memory_key_padding_mask=memory_key_padding_mask\n",
    "        )\n",
    "\n",
    "        return torch.log_softmax(self.projection(output), dim=-1)\n"
   ],
   "id": "598a607463f3621b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:50:36.323846Z",
     "start_time": "2025-04-16T15:50:28.485795Z"
    }
   },
   "cell_type": "code",
   "source": "dataset = load_dataset(\"Helsinki-NLP/opus-100\", \"en-ru\")",
   "id": "e0e9d2272c567f69",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:51:14.621067Z",
     "start_time": "2025-04-16T15:51:14.608068Z"
    }
   },
   "cell_type": "code",
   "source": "dataset[\"train\"] = dataset[\"train\"].select(range(50000))  \n",
   "id": "7b603e8e248cd3ef",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T10:31:49.632988Z",
     "start_time": "2025-04-15T10:31:49.616407Z"
    }
   },
   "cell_type": "code",
   "source": "print(dataset)",
   "id": "c48d00ca63c058fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 50000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:51:20.063165Z",
     "start_time": "2025-04-16T15:51:17.931751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import sentencepiece as spm\n",
    "from transformers import MarianTokenizer\n",
    "\n",
    "src_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-ru\")\n",
    "tgt_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-ru-en\")\n"
   ],
   "id": "2ee9a6cb676f0012",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:51:36.461202Z",
     "start_time": "2025-04-16T15:51:21.290326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    src_texts = [ex[\"en\"] for ex in examples[\"translation\"]]\n",
    "    tgt_texts = [ex[\"ru\"] for ex in examples[\"translation\"]]\n",
    "\n",
    "    model_inputs = src_tokenizer(\n",
    "        src_texts, max_length=128, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    labels = tgt_tokenizer(\n",
    "        tgt_texts, max_length=128, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n"
   ],
   "id": "731066bf8a416928",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a137bd5ba17d49d68c7416b5a52fd1cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:51:38.556608Z",
     "start_time": "2025-04-16T15:51:38.538644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "src_vocab_size = src_tokenizer.vocab_size\n",
    "tgt_vocab_size = tgt_tokenizer.vocab_size\n",
    "d_model = 256\n",
    "nhead = 4\n",
    "num_layers = 4\n",
    "batch_size = 32\n",
    "num_epochs = 100"
   ],
   "id": "26316b6e0737f579",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:51:43.555238Z",
     "start_time": "2025-04-16T15:51:40.468678Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "\n",
    "model = TransformerModel(\n",
    "    src_vocab_size=src_tokenizer.vocab_size,\n",
    "    tgt_vocab_size=tgt_tokenizer.vocab_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    num_encoder_layers=num_layers,\n",
    "    num_decoder_layers=num_layers\n",
    ")\n",
    "\n",
    "\n",
    "criterion = nn.NLLLoss(ignore_index=tgt_tokenizer.pad_token_id)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n"
   ],
   "id": "bc9a7e6438074748",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3d857df3a862d873"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:51:44.794531Z",
     "start_time": "2025-04-16T15:51:44.775115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(pytorch_total_params)"
   ],
   "id": "641e3b9a8a186ab7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39445558\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-15T10:04:23.685033Z",
     "start_time": "2025-04-15T10:04:23.670365Z"
    }
   },
   "cell_type": "code",
   "source": "print(model)",
   "id": "5914bd848b04faba",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerModel(\n",
      "  (src_embed): Sequential(\n",
      "    (0): InputEmbedding(\n",
      "      (embedding): Embedding(62518, 128)\n",
      "    )\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (tgt_embed): Sequential(\n",
      "    (0): InputEmbedding(\n",
      "      (embedding): Embedding(62518, 128)\n",
      "    )\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (transformer): Transformer(\n",
      "    (encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): TransformerDecoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-1): 2 x TransformerDecoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (multihead_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.1, inplace=False)\n",
      "          (dropout2): Dropout(p=0.1, inplace=False)\n",
      "          (dropout3): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (projection): Linear(in_features=128, out_features=62518, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:52:04.985615Z",
     "start_time": "2025-04-16T15:52:04.969381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, min_delta=0.001):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.best_loss = np.inf\n",
    "        self.counter = 0\n",
    "\n",
    "    def step(self, val_loss):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        return self.counter >= self.patience"
   ],
   "id": "39237b1fea319013",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:52:11.179899Z",
     "start_time": "2025-04-16T15:52:08.176734Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ],
   "id": "e43c296ca1cfaf2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerModel(\n",
       "  (src_embed): Sequential(\n",
       "    (0): InputEmbedding(\n",
       "      (embedding): Embedding(62518, 256)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (tgt_embed): Sequential(\n",
       "    (0): InputEmbedding(\n",
       "      (embedding): Embedding(62518, 256)\n",
       "    )\n",
       "    (1): PositionalEncoding(\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (transformer): Transformer(\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerEncoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): TransformerDecoder(\n",
       "      (layers): ModuleList(\n",
       "        (0-3): 4 x TransformerDecoderLayer(\n",
       "          (self_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (multihead_attn): MultiheadAttention(\n",
       "            (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "          )\n",
       "          (linear1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "          (linear2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout1): Dropout(p=0.1, inplace=False)\n",
       "          (dropout2): Dropout(p=0.1, inplace=False)\n",
       "          (dropout3): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (projection): Linear(in_features=256, out_features=62518, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:52:13.157933Z",
     "start_time": "2025-04-16T15:52:13.150660Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def log(msg):\n",
    "    print(msg)\n",
    "    log_file.write(msg + \"\\n\")\n",
    "    log_file.flush()\n",
    "\n",
    "\n",
    "def generate_square_subsequent_mask(sz):\n",
    "    return torch.triu(torch.ones(sz, sz) * float(\"-inf\"), diagonal=1)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src = torch.tensor([x[\"input_ids\"] for x in batch], dtype=torch.long)\n",
    "    tgt = torch.tensor([x[\"labels\"] for x in batch], dtype=torch.long)\n",
    "    return src.to(device), tgt.to(device)\n"
   ],
   "id": "95a5476ea9ad07a6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:52:14.712615Z",
     "start_time": "2025-04-16T15:52:14.706131Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(tokenized_datasets[\"validation\"], batch_size=batch_size, collate_fn=collate_fn)\n",
    "\n"
   ],
   "id": "af7fb003e7c87d60",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T15:52:17.125980Z",
     "start_time": "2025-04-16T15:52:17.103233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.cuda.amp import GradScaler, autocast\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "scaler = GradScaler()\n",
    "log_file = open(\"training_log.txt\", \"w\", encoding=\"utf-8\")\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "def train(model, criterion, optimizer, num_epochs=10, device=\"cuda\", log_interval=1, train_dataloader = train_dataloader, val_dataloader = val_dataloader, patience=7):\n",
    "    train_losses, val_losses = [], []\n",
    "    best_val_loss = float(\"inf\")\n",
    "    early_stopping = EarlyStopping(patience=patience)\n",
    "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "\n",
    "    log_file = open(\"training.log\", \"w\")\n",
    "\n",
    "    def log(msg):\n",
    "        print(msg)\n",
    "        log_file.write(msg + \"\\n\")\n",
    "        log_file.flush()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        total_tokens = 0\n",
    "\n",
    "        for batch_idx, (src, tgt) in enumerate(train_dataloader):\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "\n",
    "            tgt_tokenizer.bos_token_id = 1\n",
    "            bos_id = tgt_tokenizer.bos_token_id\n",
    "            tgt_input = torch.cat([\n",
    "                torch.full((tgt.size(0), 1), bos_id, dtype=torch.long, device=device),\n",
    "                tgt[:, :-1]\n",
    "            ], dim=1)\n",
    "\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "\n",
    "            tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "            src_padding_mask = (src == src_tokenizer.pad_token_id)\n",
    "            tgt_padding_mask = (tgt_input == tgt_tokenizer.pad_token_id)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.autocast(device_type='cuda'):\n",
    "                output = model(\n",
    "                    src, tgt_input,\n",
    "                    tgt_mask=tgt_mask,\n",
    "                    src_key_padding_mask=src_padding_mask,\n",
    "                    tgt_key_padding_mask=tgt_padding_mask,\n",
    "                )\n",
    "                output = output[:, :tgt_output.size(1), :]\n",
    "                loss = criterion(output.reshape(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            non_pad_tokens = (tgt_output != tgt_tokenizer.pad_token_id).sum().item()\n",
    "            total_tokens += non_pad_tokens\n",
    "            total_loss += loss.item() * non_pad_tokens\n",
    "\n",
    "            if batch_idx % log_interval == 0:\n",
    "                log(\n",
    "                    f\"[Эпоха {epoch+1}/{num_epochs}] \"\n",
    "                    f\"[Батч {batch_idx+1}/{len(train_dataloader)}] \"\n",
    "                    f\"Потеря: {loss.item():.4f} | Средняя потеря: {total_loss/total_tokens:.4f}\"\n",
    "                )\n",
    "                log_file.flush()\n",
    "\n",
    "        avg_train_loss = total_loss / total_tokens\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in val_dataloader:\n",
    "                src, tgt = batch\n",
    "                src = src.to(device)\n",
    "                tgt = tgt.to(device)\n",
    "\n",
    "                bos_id = tgt_tokenizer.bos_token_id\n",
    "                tgt_input = torch.cat([\n",
    "                    torch.full((tgt.size(0), 1), bos_id, dtype=torch.long, device=device),\n",
    "                    tgt[:, :-1]\n",
    "                ], dim=1)\n",
    "                tgt_output = tgt[:, 1:]\n",
    "                tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(device)\n",
    "\n",
    "                src_padding_mask = (src == src_tokenizer.pad_token_id)\n",
    "                tgt_padding_mask = (tgt_input == tgt_tokenizer.pad_token_id)\n",
    "\n",
    "                output = model(\n",
    "                    src, tgt_input,\n",
    "                    tgt_mask=tgt_mask,\n",
    "                    src_key_padding_mask=src_padding_mask,\n",
    "                    tgt_key_padding_mask=tgt_padding_mask\n",
    "                )\n",
    "                loss = criterion(output.view(-1, output.size(-1)), tgt_output.reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        log(f\"Эпоха {epoch+1} все: Train loss = {avg_train_loss:.4f}, Val loss = {avg_val_loss:.4f}\")\n",
    "        log_file.flush()\n",
    "\n",
    "        if early_stopping.step(val_loss):\n",
    "            print(f\"Стоп  обучения на эпохе {epoch+1} (без улучшений {patience} эпох)\")\n",
    "            break\n",
    "\n",
    "        checkpoint_path = f\"checkpoints/epoch_{epoch+1}.pt\"\n",
    "        torch.save({\n",
    "            \"epoch\": epoch,\n",
    "            \"model_state_dict\": model.state_dict(),\n",
    "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "            \"train_loss\": train_losses,\n",
    "            \"val_loss\": val_losses,\n",
    "        }, checkpoint_path)\n",
    "        log(f\"Сохранено: {checkpoint_path}\")\n",
    "        log_file.flush()\n",
    "\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"checkpoints/best_model.pt\")\n",
    "            log(\"Сохранена новая модель\")\n",
    "            log_file.flush()\n",
    "\n",
    "    log_file.close()\n",
    "    return train_losses, val_losses"
   ],
   "id": "a30dae1f4f11709c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Zina\\AppData\\Local\\Temp\\ipykernel_8140\\3013954055.py:6: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-16T16:01:33.405393Z",
     "start_time": "2025-04-16T15:59:21.583578Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_losses, val_losses = train(\n",
    "    model=model, \n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloader=val_dataloader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    num_epochs=num_epochs,\n",
    "    device=device,\n",
    "    log_interval=1\n",
    ")"
   ],
   "id": "c410c186a9968267",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Эпоха 1/100] [Батч 1/1563] Потеря: 7.8852 | Средняя потеря: 7.8852\n",
      "[Эпоха 1/100] [Батч 2/1563] Потеря: 8.1183 | Средняя потеря: 8.0119\n",
      "[Эпоха 1/100] [Батч 3/1563] Потеря: 7.8970 | Средняя потеря: 7.9787\n",
      "[Эпоха 1/100] [Батч 4/1563] Потеря: 8.0195 | Средняя потеря: 7.9901\n",
      "[Эпоха 1/100] [Батч 5/1563] Потеря: 7.7092 | Средняя потеря: 7.9380\n",
      "[Эпоха 1/100] [Батч 6/1563] Потеря: 7.9387 | Средняя потеря: 7.9381\n",
      "[Эпоха 1/100] [Батч 7/1563] Потеря: 7.7699 | Средняя потеря: 7.9192\n",
      "[Эпоха 1/100] [Батч 8/1563] Потеря: 7.8881 | Средняя потеря: 7.9146\n",
      "[Эпоха 1/100] [Батч 9/1563] Потеря: 8.0773 | Средняя потеря: 7.9352\n",
      "[Эпоха 1/100] [Батч 10/1563] Потеря: 8.0103 | Средняя потеря: 7.9455\n",
      "[Эпоха 1/100] [Батч 11/1563] Потеря: 7.9267 | Средняя потеря: 7.9440\n",
      "[Эпоха 1/100] [Батч 12/1563] Потеря: 7.8797 | Средняя потеря: 7.9377\n",
      "[Эпоха 1/100] [Батч 13/1563] Потеря: 8.0557 | Средняя потеря: 7.9471\n",
      "[Эпоха 1/100] [Батч 14/1563] Потеря: 7.7477 | Средняя потеря: 7.9336\n",
      "[Эпоха 1/100] [Батч 15/1563] Потеря: 7.8629 | Средняя потеря: 7.9295\n",
      "[Эпоха 1/100] [Батч 16/1563] Потеря: 7.6959 | Средняя потеря: 7.9171\n",
      "[Эпоха 1/100] [Батч 17/1563] Потеря: 7.7011 | Средняя потеря: 7.9024\n",
      "[Эпоха 1/100] [Батч 18/1563] Потеря: 7.8051 | Средняя потеря: 7.8962\n",
      "[Эпоха 1/100] [Батч 19/1563] Потеря: 7.9479 | Средняя потеря: 7.8985\n",
      "[Эпоха 1/100] [Батч 20/1563] Потеря: 7.4861 | Средняя потеря: 7.8823\n",
      "[Эпоха 1/100] [Батч 21/1563] Потеря: 7.9226 | Средняя потеря: 7.8848\n",
      "[Эпоха 1/100] [Батч 22/1563] Потеря: 7.9736 | Средняя потеря: 7.8892\n",
      "[Эпоха 1/100] [Батч 23/1563] Потеря: 7.5043 | Средняя потеря: 7.8756\n",
      "[Эпоха 1/100] [Батч 24/1563] Потеря: 7.5088 | Средняя потеря: 7.8646\n",
      "[Эпоха 1/100] [Батч 25/1563] Потеря: 7.8356 | Средняя потеря: 7.8635\n",
      "[Эпоха 1/100] [Батч 26/1563] Потеря: 7.5879 | Средняя потеря: 7.8551\n",
      "[Эпоха 1/100] [Батч 27/1563] Потеря: 7.7496 | Средняя потеря: 7.8516\n",
      "[Эпоха 1/100] [Батч 28/1563] Потеря: 7.5861 | Средняя потеря: 7.8415\n",
      "[Эпоха 1/100] [Батч 29/1563] Потеря: 7.3162 | Средняя потеря: 7.8258\n",
      "[Эпоха 1/100] [Батч 30/1563] Потеря: 7.4614 | Средняя потеря: 7.8155\n",
      "[Эпоха 1/100] [Батч 31/1563] Потеря: 7.7878 | Средняя потеря: 7.8146\n",
      "[Эпоха 1/100] [Батч 32/1563] Потеря: 7.5326 | Средняя потеря: 7.8065\n",
      "[Эпоха 1/100] [Батч 33/1563] Потеря: 7.8289 | Средняя потеря: 7.8072\n",
      "[Эпоха 1/100] [Батч 34/1563] Потеря: 7.5438 | Средняя потеря: 7.7995\n",
      "[Эпоха 1/100] [Батч 35/1563] Потеря: 7.6867 | Средняя потеря: 7.7968\n",
      "[Эпоха 1/100] [Батч 36/1563] Потеря: 7.6311 | Средняя потеря: 7.7933\n",
      "[Эпоха 1/100] [Батч 37/1563] Потеря: 7.7332 | Средняя потеря: 7.7909\n",
      "[Эпоха 1/100] [Батч 38/1563] Потеря: 7.7878 | Средняя потеря: 7.7908\n",
      "[Эпоха 1/100] [Батч 39/1563] Потеря: 7.3933 | Средняя потеря: 7.7842\n",
      "[Эпоха 1/100] [Батч 40/1563] Потеря: 7.5861 | Средняя потеря: 7.7798\n",
      "[Эпоха 1/100] [Батч 41/1563] Потеря: 7.7374 | Средняя потеря: 7.7786\n",
      "[Эпоха 1/100] [Батч 42/1563] Потеря: 7.6916 | Средняя потеря: 7.7764\n",
      "[Эпоха 1/100] [Батч 43/1563] Потеря: 7.7225 | Средняя потеря: 7.7753\n",
      "[Эпоха 1/100] [Батч 44/1563] Потеря: 7.7325 | Средняя потеря: 7.7744\n",
      "[Эпоха 1/100] [Батч 45/1563] Потеря: 6.9491 | Средняя потеря: 7.7637\n",
      "[Эпоха 1/100] [Батч 46/1563] Потеря: 7.6340 | Средняя потеря: 7.7611\n",
      "[Эпоха 1/100] [Батч 47/1563] Потеря: 7.5219 | Средняя потеря: 7.7567\n",
      "[Эпоха 1/100] [Батч 48/1563] Потеря: 7.5745 | Средняя потеря: 7.7530\n",
      "[Эпоха 1/100] [Батч 49/1563] Потеря: 7.6651 | Средняя потеря: 7.7510\n",
      "[Эпоха 1/100] [Батч 50/1563] Потеря: 7.7477 | Средняя потеря: 7.7509\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m train_losses, val_losses = \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m      2\u001B[39m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\n\u001B[32m      3\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtrain_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      4\u001B[39m \u001B[43m    \u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m=\u001B[49m\u001B[43mval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      5\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m=\u001B[49m\u001B[43mcriterion\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      6\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m=\u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      7\u001B[39m \u001B[43m    \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m=\u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      8\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m      9\u001B[39m \u001B[43m    \u001B[49m\u001B[43mlog_interval\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\n\u001B[32m     10\u001B[39m \u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 57\u001B[39m, in \u001B[36mtrain\u001B[39m\u001B[34m(model, criterion, optimizer, num_epochs, device, log_interval, train_dataloader, val_dataloader, patience)\u001B[39m\n\u001B[32m     54\u001B[39m     output = output[:, :tgt_output.size(\u001B[32m1\u001B[39m), :]\n\u001B[32m     55\u001B[39m     loss = criterion(output.reshape(-\u001B[32m1\u001B[39m, output.size(-\u001B[32m1\u001B[39m)), tgt_output.reshape(-\u001B[32m1\u001B[39m))\n\u001B[32m---> \u001B[39m\u001B[32m57\u001B[39m \u001B[43mscaler\u001B[49m\u001B[43m.\u001B[49m\u001B[43mscale\u001B[49m\u001B[43m(\u001B[49m\u001B[43mloss\u001B[49m\u001B[43m)\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     58\u001B[39m scaler.step(optimizer)\n\u001B[32m     59\u001B[39m scaler.update()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\DopML\\pythonProject\\.venv\\Lib\\site-packages\\torch\\_tensor.py:626\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    616\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    617\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    618\u001B[39m         Tensor.backward,\n\u001B[32m    619\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    624\u001B[39m         inputs=inputs,\n\u001B[32m    625\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m626\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    628\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\DopML\\pythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    342\u001B[39m     retain_graph = create_graph\n\u001B[32m    344\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    345\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    346\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m347\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    348\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    349\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    350\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    351\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    352\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    353\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    354\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\DopML\\pythonProject\\.venv\\Lib\\site-packages\\torch\\autograd\\graph.py:823\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    821\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    822\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m823\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    824\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    825\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    826\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    827\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 18
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
